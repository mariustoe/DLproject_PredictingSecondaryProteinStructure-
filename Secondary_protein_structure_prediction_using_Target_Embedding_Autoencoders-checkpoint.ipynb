{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secondary protein structure prediction using Target Embedding Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import sklearn.model_selection as model_selection\n",
    "import pickle\n",
    "from typing import *\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display, clear_output\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "sns.set_style(\"whitegrid\")\n",
    "try:\n",
    "    from plotting import make_vae_plots\n",
    "except Exception as ex:\n",
    "    print(f\"If using Colab, you may need to upload `plotting.py`. \\\n",
    "          \\nIn the left pannel, click `Files > upload to session storage` and select the file `plotting.py` from your computer \\\n",
    "          \\n---------------------------------------------\")\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "# Insert your path to dataset (drive_path)\n",
    "drive_path = 'C:/Users/augus/OneDrive - Danmarks Tekniske Universitet/Studiemappe/9. semester/Deep Learning/project/'\n",
    "traindata = pickle.load(open(drive_path+'train_reduced.pickle','rb'))\n",
    "X = traindata[0][:,:,0:20]\n",
    "y = traindata[0][:,:,57:65]\n",
    "CASP12data = pickle.load(open(drive_path+'Casp12Data.pickle','rb'))\n",
    "X_casp = CASP12data[0][:,:,0:20]\n",
    "y_casp = CASP12data[0][:,:,57:65]\n",
    "TS115data = pickle.load(open(drive_path+'TS115.pickle','rb'))\n",
    "X_TS115 = TS115data[0][:,:,0:20]\n",
    "y_TS115 = TS115data[0][:,:,57:65]\n",
    "CB513data = pickle.load(open(drive_path+'CB513.pickle','rb'))\n",
    "X_CB513 = CB513data[0][:,:,0:20]\n",
    "y_CB513 = CB513data[0][:,:,57:65]\n",
    "labels = traindata[1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-aaf329e8cf45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# As the lengths of the sequences are extrapolated to match the length of the longest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mEPzeros\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mEPzeros_casp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_casp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mEPzeros_TS115\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_TS115\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "# As the lengths of the sequences are extrapolated to match the length of the longest\n",
    "# sequence in the dataset. Therefore, we have to add an artificial class so that the extrapolated class is not mistaken for the first class.\n",
    "if y.shape[2] == 8:\n",
    "    EPzeros = np.expand_dims(np.zeros((y.shape[1])),1)\n",
    "    EPzeros_casp = np.expand_dims(np.zeros((y_casp.shape[1])),1)\n",
    "    EPzeros_TS115 = np.expand_dims(np.zeros((y_TS115.shape[1])),1)\n",
    "    EPzeros_CB513 = np.expand_dims(np.zeros((y_CB513.shape[1])),1)\n",
    "    y = np.asarray([np.hstack((y[i],EPzeros)) for i in range(y.shape[0])])\n",
    "    y_casp = np.asarray([np.hstack((y_casp[i],EPzeros_casp)) for i in range(y_casp.shape[0])])\n",
    "    y_TS115 = np.asarray([np.hstack((y_TS115[i],EPzeros_TS115)) for i in range(y_TS115.shape[0])])\n",
    "    y_CB513 = np.asarray([np.hstack((y_CB513[i],EPzeros_CB513)) for i in range(y_CB513.shape[0])])\n",
    "    for i in range(y.shape[0]):\n",
    "        for j in range(y[i].shape[0]):\n",
    "            if np.all(y[i][j,:] == 0):\n",
    "                y[i][j,:][-1] = 1\n",
    "    for i in range(y_casp.shape[0]):\n",
    "        for j in range(y_casp[i].shape[0]):\n",
    "            if np.all(y_casp[i][j,:] == 0):\n",
    "                y_casp[i][j,:][-1] = 1\n",
    "    for i in range(y_TS115.shape[0]):\n",
    "        for j in range(y_TS115[i].shape[0]):\n",
    "            if np.all(y_TS115[i][j,:] == 0):\n",
    "                y_TS115[i][j,:][-1] = 1\n",
    "    for i in range(y_CB513.shape[0]):\n",
    "        for j in range(y_CB513[i].shape[0]):\n",
    "            if np.all(y_CB513[i][j,:] == 0):\n",
    "                y_CB513[i][j,:][-1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting arrays to tensors for PyTorch\n",
    "X_train = torch.tensor(X, dtype = torch.float)\n",
    "y_train = torch.tensor(y, dtype = torch.float).permute(0,2,1)\n",
    "X_casp = torch.tensor(X_casp,dtype = torch.float)\n",
    "y_casp = torch.tensor(y_casp,dtype=torch.float).permute(0,2,1)\n",
    "X_TS115 = torch.tensor(X_TS115,dtype=torch.float)\n",
    "y_TS115 = torch.tensor(y_TS115,dtype=torch.float).permute(0,2,1)\n",
    "X_CB513 = torch.tensor(X_CB513,dtype=torch.float)\n",
    "y_CB513 = torch.tensor(y_CB513,dtype=torch.float).permute(0,2,1)\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.transforms import ToTensor\n",
    "from functools import reduce\n",
    "\n",
    "# Train in batch sizes of 65 and evaluate on batch size of 65\n",
    "batch_size = 65\n",
    "eval_batch_size = 65\n",
    "# The loaders perform the actual work\n",
    "train_loader = DataLoader(y_train, batch_size=batch_size)\n",
    "casp_loader = DataLoader(y_casp,batch_size=3)\n",
    "TS115_loader = DataLoader(y_TS115,batch_size=batch_size)\n",
    "CB513_loader = DataLoader(y_CB513,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 training: Variational Autoencoder (VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the reparameterization trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn.functional import softplus\n",
    "from torch.distributions import Distribution\n",
    "\n",
    "class ReparameterizedDiagonalGaussian(Distribution):\n",
    "    \"\"\"\n",
    "    A distribution `N(y | mu, sigma I)` compatible with the reparameterization trick given `epsilon ~ N(0, 1)`.\n",
    "    \"\"\"\n",
    "    def __init__(self, mu: Tensor, log_sigma:Tensor):\n",
    "        assert mu.shape == log_sigma.shape, f\"Tensors `mu` : {mu.shape} and ` log_sigma` : {log_sigma.shape} must be of the same shape\"\n",
    "        self.mu = mu\n",
    "        self.sigma = log_sigma.exp()\n",
    "        \n",
    "    def sample_epsilon(self) -> Tensor:\n",
    "        \"\"\"`\\eps ~ N(0, I)`\"\"\"\n",
    "        return torch.empty_like(self.mu).normal_()\n",
    "        \n",
    "    def sample(self) -> Tensor:\n",
    "        \"\"\"sample `z ~ N(z | mu, sigma)` (without gradients)\"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.rsample()\n",
    "        \n",
    "    def rsample(self) -> Tensor:\n",
    "        \"\"\"sample `z ~ N(z | mu, sigma)` (with the reparameterization trick) \"\"\"\n",
    "        return self.mu + self.sigma * self.sample_epsilon() # your code\n",
    "        \n",
    "        \n",
    "    def log_prob(self, z:Tensor) -> Tensor:\n",
    "        \"\"\"return the log probability: log `p(z)`\"\"\"\n",
    "        logprob = -0.5 * math.log(2 * math.pi) - self.sigma.log() - 0.5 * ((z - self.mu)/self.sigma)**2\n",
    "        return logprob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numbers import Number\n",
    "import math\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "from torch.distributions.uniform import Uniform\n",
    "from torch.distributions.transformed_distribution import TransformedDistribution\n",
    "from torch.distributions.transforms import AffineTransform, ExpTransform\n",
    "from torch.distributions.utils import broadcast_all\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "\n",
    "channels = y_train.shape[1]\n",
    "kernel_size_conv1 = 9\n",
    "stride_conv1 = 1\n",
    "padding_conv1 = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_shape:torch.Size, latent_features:int) -> None:\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.latent_features_cat = latent_features_cat\n",
    "        self.observation_features =  self.input_shape\n",
    "\n",
    "        # Inference Network\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=channels, out_channels=2*latent_features_cat, kernel_size=kernel_size_conv1, stride=stride_conv1, padding=padding_conv1,bias=True),\n",
    "            nn.Softmax(dim = 1)\n",
    "        )\n",
    "        # Generative Model\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=latent_features_cat, out_channels=9, kernel_size=kernel_size_conv1, stride=stride_conv1, padding=padding_conv1,bias=True)\n",
    "        )\n",
    "        \n",
    "        # define the parameters of the prior, chosen as p(z) = N(0, I)\n",
    "        self.register_buffer('prior_params', torch.zeros(torch.Size([1, 2*latent_features_cat])))\n",
    "\n",
    "    def posterior(self, x:Tensor) -> Distribution:\n",
    "        \"\"\"return the distribution `q(x|x) = N(z | \\mu(x), \\sigma(x))`\"\"\"\n",
    "        \n",
    "        # compute the parameters of the posterior\n",
    "        h_x = self.encoder(x)\n",
    "        h_x = h_x.permute(1,2,0).flatten(start_dim=1).permute(1,0)\n",
    "        mu, log_sigma =  h_x.chunk(2, dim=-1)\n",
    "        # return a distribution `q(x|x) = N(z | \\mu(x), \\sigma(x))`\n",
    "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
    "    \n",
    "    def prior(self, batch_size:int=1)-> Distribution:\n",
    "        \"\"\"return the distribution `p(z)`\"\"\"\n",
    "        prior_params = self.prior_params.expand(batch_size, *self.prior_params.shape[-1:])\n",
    "        mu, log_sigma = prior_params.chunk(2, dim=-1)        \n",
    "        # return the distribution `p(z)`\n",
    "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
    "    \n",
    "    def observation_model(self, z:Tensor) -> Distribution:\n",
    "        \"\"\"return the distribution `p(x|z)`\"\"\"\n",
    "        px_logits1 = self.decoder(z.permute(0,2,1))\n",
    "        px_logits = px_logits1\n",
    "        px_logits = sigmoid(px_logits.permute(0,2,1))\n",
    "        px_logits = px_logits.reshape(px_logits.shape[0]*px_logits.shape[1],-1)\n",
    "        return Bernoulli(logits=px_logits), px_logits1\n",
    "        \n",
    "\n",
    "    def forward(self, x) -> Dict[str, Any]:\n",
    "        \"\"\"compute the posterior q(z|x) (encoder), sample z~q(z|x) and return the distribution p(x|z) (decoder)\"\"\"\n",
    "\n",
    "        # define the posterior q(z|x) / encode x into q(z|x)\n",
    "        x = x.permute(0,2,1)\n",
    "        qz = self.posterior(x)\n",
    "        \n",
    "        # define the prior p(z)\n",
    "        pz = self.prior(batch_size=x.size(0)*x.size(2))\n",
    "        \n",
    "        # sample the posterior using the reparameterization trick: z ~ q(z | x)\n",
    "        z = qz.rsample()\n",
    "        \n",
    "        # define the observation model p(x|z) = B(x | g(z))\n",
    "        px = self.observation_model(z.view(x.size(0),x.size(2),-1))[0]\n",
    "        \n",
    "        return {'px': px, 'pz': pz, 'qz': qz, 'z': z}\n",
    "    \n",
    "    \n",
    "    def sample_from_prior(self, batch_size:int=100):\n",
    "        \"\"\"sample z~p(z) and return p(x|z)\"\"\"\n",
    "        \n",
    "        # degine the prior p(z)\n",
    "        pz = self.prior(batch_size=batch_size)\n",
    "        \n",
    "        # sample the prior \n",
    "        z = pz.rsample()\n",
    "        \n",
    "        # define the observation model p(x|z) = B(x | g(z))\n",
    "        px = self.observation_model(z)[0]\n",
    "        \n",
    "        return {'px': px, 'pz': pz, 'z': z}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating variational inference module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(x:Tensor) -> Tensor:\n",
    "    \"\"\"for each datapoint: sum over all dimensions\"\"\"\n",
    "    return x.view(x.size(0), -1).sum(dim=1)\n",
    "\n",
    "class VariationalInference(nn.Module):\n",
    "    def __init__(self, beta:float=1.):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(self, model:nn.Module, x:Tensor) -> Tuple[Tensor, Dict]:\n",
    "        \n",
    "        # forward pass through the model\n",
    "        outputs = model(x)\n",
    "        # unpack outputs\n",
    "        px, pz, qz, z = [outputs[k] for k in [\"px\", \"pz\", \"qz\", \"z\"]]\n",
    "        \n",
    "        # evaluate log probabilities\n",
    "        log_px = reduce(px.log_prob(x.permute(2,1,0).flatten(start_dim=1).permute(1,0)))\n",
    "        log_px = log_px.view(x.size(0),x.size(1))\n",
    "        log_pz = reduce(pz.log_prob(z))\n",
    "        log_qz = reduce(qz.log_prob(z))\n",
    "        #print(\"log_px forward\", log_px.shape, \"log_pz forward\", log_pz.shape, \"log_qz forward\", log_qz.shape)\n",
    "\n",
    "        # compute the ELBO with and without the beta parameter: \n",
    "        # `L^\\beta = E_q [ log p(x|z) - \\beta * D_KL(q(z|x) | p(z))`\n",
    "        # where `D_KL(q(z|x) | p(z)) = log q(z|x) - log p(z)`\n",
    "        kl = log_qz - log_pz\n",
    "        kl=kl.view(x.size(0),x.size(1))\n",
    "        elbo = log_px - kl# <- your code here\n",
    "        beta_elbo = log_px - self.beta * kl# <- your code here\n",
    "        \n",
    "        # loss\n",
    "        loss = -beta_elbo.mean()\n",
    "        \n",
    "        # prepare the output\n",
    "        with torch.no_grad():\n",
    "            diagnostics = {'elbo': beta_elbo, 'log_px':log_px, 'kl': kl}\n",
    "            \n",
    "        return loss, diagnostics, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# define the models, evaluator and optimizer\n",
    "\n",
    "# Initializing the VAE with the 3 latent variables\n",
    "# We choose to project the 8 classes into a three dimensional latent space.\n",
    "latent_features_cat = 3\n",
    "vae = VariationalAutoencoder(y_train.shape[1], latent_features_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator: Variational Inference\n",
    "beta = 0.04\n",
    "vi = VariationalInference(beta=beta)\n",
    "\n",
    "# The Adam optimizer works well with VAEs.\n",
    "optimizer_vae = torch.optim.Adam(vae.parameters(), lr=1e-3,betas=(0.85,0.95),weight_decay=1e-6)\n",
    "\n",
    "# define dictionary to store the training curves\n",
    "training_data = defaultdict(list)\n",
    "casp_data = defaultdict(list)\n",
    "TS115_data = defaultdict(list)\n",
    "CB513_data = defaultdict(list)\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training..\n",
    "while epoch < num_epochs:\n",
    "    epoch+= 1\n",
    "    print(\"Epoch \",epoch,\" of \",num_epochs)\n",
    "    training_epoch_data = defaultdict(list)\n",
    "    vae.train()\n",
    "    \n",
    "    # Go through each batch in the training dataset using the loader\n",
    "    # Note that y is not necessarily known as it is here\n",
    "    for x in train_loader:\n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        loss, diagnostics, outputs = vi(vae, x.permute(0,2,1))\n",
    "        \n",
    "        optimizer_vae.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_vae.step()\n",
    "        \n",
    "        # gather data for the current bach\n",
    "        for k, v in diagnostics.items():\n",
    "            training_epoch_data[k] += [v.mean().item()]\n",
    "            \n",
    "\n",
    "    # gather data for the full epoch\n",
    "    for k, v in training_epoch_data.items():\n",
    "        training_data[k] += [np.mean(training_epoch_data[k])]\n",
    "\n",
    "    # Evaluate on a single batch, do not propagate gradients\n",
    "    with torch.no_grad():\n",
    "        vae.eval()\n",
    "        # Load a single batch from the test loader\n",
    "        x_casp = next(iter(casp_loader))\n",
    "        \n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        loss_casp, diagnostics_casp, outputs_casp = vi(vae, x_casp.permute(0,2,1))\n",
    "        \n",
    "        # gather data for the validation step\n",
    "        for k, v in diagnostics_casp.items():\n",
    "            casp_data[k] += [v.mean().item()]\n",
    "        # Just load a single batch from the test loader\n",
    "        x_TS115 = next(iter(TS115_loader))\n",
    "        \n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        loss_TS115, diagnostics_TS115, outputs_TS115 = vi(vae, x_TS115.permute(0,2,1))\n",
    "        \n",
    "        # gather data for the validation step\n",
    "        for k, v in diagnostics_TS115.items():\n",
    "            TS115_data[k] += [v.mean().item()]\n",
    "        # Just load a single batch from the test loader\n",
    "        x_CB513 = next(iter(CB513_loader))\n",
    "        x_CB513 = x_CB513.to(device)\n",
    "        \n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        loss_CB513, diagnostics_CB513, outputs_CB513 = vi(vae, x_CB513.permute(0,2,1))\n",
    "        \n",
    "        # gather data for the validation step\n",
    "        for k, v in diagnostics_CB513.items():\n",
    "            CB513_data[k] += [v.mean().item()]\n",
    "    \n",
    "    # Reproduce the figure from the begining of the notebook, plot the training curves and show latent samples\n",
    "    make_vae_plots(vae, X, y, outputs, training_data, casp_data,TS115_data,CB513_data)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we have 20 different amino acids we have 20 channels\n",
    "channels = 20\n",
    "# Defining sizes for the first layer\n",
    "kernel_size_conv1 = 15\n",
    "padding_conv1 = 7\n",
    "stride_conv1 = 1\n",
    "# second layer\n",
    "kernel_size_conv2 = 9\n",
    "padding_conv2 = 4\n",
    "stride_conv2 = 1\n",
    "# Third layer\n",
    "kernel_size_conv3 = 5\n",
    "padding_conv3 = 2\n",
    "stride_conv3 = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using dropout and batchnorm in all layers and ReLU except in the dense layer where sigmoid is implemented\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=channels,out_channels=25,kernel_size=kernel_size_conv1 , stride=stride_conv1, padding=padding_conv1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(25),\n",
    "            nn.Dropout(p=0.5))\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=25, out_channels=35, kernel_size=kernel_size_conv2, stride=stride_conv2, padding=padding_conv2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(35),\n",
    "            nn.Dropout(p=0.5))\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=35,out_channels=40, kernel_size=kernel_size_conv3, stride=stride_conv3, padding=padding_conv3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(40),\n",
    "            nn.Dropout(p=0.5))\n",
    "\n",
    "        self.fc1_encode1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=40,out_channels=latent_features_cat,kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.Sigmoid(),\n",
    "            nn.BatchNorm1d(latent_features_cat)\n",
    "        )\n",
    "    # Forwarding the data\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.fc1_encode1(x)\n",
    "        return x\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining criterion and optimizer\n",
    "import torch.optim as optim\n",
    "# We use the MSELoss function as we have a continuous output.\n",
    "criterion1 = nn.MSELoss()\n",
    "# Again, we use the Adam optimizer\n",
    "optimizer_net = optim.Adam(net.parameters(), lr=0.001,betas=(0.85,0.95),weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X\n",
    "        self.targets = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "      x = self.data[index]\n",
    "      y = self.targets[index]\n",
    "      return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "      return len(self.data)\n",
    "batch_size = 65\n",
    "TrainLoader = DataLoader(MyDataset(X_train,y_train),batch_size=batch_size)\n",
    "CASPLoader = DataLoader(MyDataset(X_casp,y_casp),batch_size=3)\n",
    "TS115Loader = DataLoader(MyDataset(X_TS115,y_TS115),batch_size=batch_size)\n",
    "CB513Loader = DataLoader(MyDataset(X_CB513,y_CB513),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating list to store training and test losses and accuracies\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "casp_loss = []\n",
    "casp_accuracy = []\n",
    "TS115_loss = []\n",
    "TS115_accuracy = []\n",
    "CB513_loss = []\n",
    "CB513_accuracy = []\n",
    "### Training\n",
    "num_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to use the encoder from the VAE to encode the targets into latent features.\n",
    "# We don¨t want the VAE to train in this step. therefore we \"freeze\" it.\n",
    "vae.eval()\n",
    "# Training\n",
    "for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
    "    print('Epoch ',epoch+1,' of ',num_epoch)\n",
    "    running_loss = 0.0\n",
    "    net.train()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for data in TrainLoader:\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        # Encoding the target values\n",
    "        outputs = vae(labels.permute(0,2,1))\n",
    "        z = outputs[\"z\"]\n",
    "        inputs, labels_latent = Variable(inputs.permute(0,2,1)), Variable(z)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer_net.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        outputs = outputs.permute(1,0,2)\n",
    "        outputs = torch.flatten(outputs, start_dim=1)\n",
    "        outputs = outputs.permute(1,0)\n",
    "        # Calculating the loss between the actual latent variables and predicted latent variables.\n",
    "        loss = criterion1(outputs,labels_latent)\n",
    "        loss.backward()\n",
    "        running_loss += loss.data.numpy()\n",
    "        optimizer_net.step()\n",
    "    train_loss.append(running_loss/len(TrainLoader))\n",
    "    # Validating on test set\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.0\n",
    "        print('Evaluating on CASP')\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in CASPLoader:\n",
    "            inputs, labels = data\n",
    "            outputs = vae(labels.permute(0,2,1))\n",
    "            z = outputs[\"z\"]\n",
    "            labels_latent = z\n",
    "            #print(labels)\n",
    "            outputs = net(Variable(inputs.permute(0,2,1)))\n",
    "            outputs = outputs.permute(1,0,2)\n",
    "            outputs = torch.flatten(outputs, start_dim=1)\n",
    "            outputs = outputs.permute(1,0)\n",
    "            running_loss += criterion1(outputs,labels_latent)\n",
    "        casp_loss.append(running_loss.data.numpy()/len(CASPLoader))\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        print('Evaluating on TS115')\n",
    "        for data in TS115Loader:\n",
    "            inputs, labels = data\n",
    "            outputs = vae(labels.permute(0,2,1))\n",
    "            z = outputs[\"z\"]\n",
    "            labels_latent = z\n",
    "            outputs = net(Variable(inputs.permute(0,2,1)))\n",
    "            outputs = outputs.permute(1,0,2)\n",
    "            outputs = torch.flatten(outputs, start_dim=1)\n",
    "            outputs = outputs.permute(1,0)\n",
    "            running_loss += criterion1(outputs,labels_latent)\n",
    "        TS115_loss.append(running_loss.data.numpy()/len(TS115Loader))\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        print('Evaluating on CB513')\n",
    "        for data in CB513Loader:\n",
    "            inputs, labels = data\n",
    "            outputs = vae(labels.permute(0,2,1))\n",
    "            z = outputs[\"z\"]\n",
    "            labels_latent = z\n",
    "            outputs = net(Variable(inputs.permute(0,2,1)))\n",
    "            outputs = outputs.permute(1,0,2)\n",
    "            outputs = torch.flatten(outputs, start_dim=1)\n",
    "            outputs = outputs.permute(1,0)\n",
    "            running_loss += criterion1(outputs,labels_latent)\n",
    "        CB513_loss.append(running_loss.data.numpy()/len(CB513Loader))\n",
    "plt.plot(range(1,num_epoch+1),train_loss,label='Train')\n",
    "plt.plot(range(1,num_epoch+1),casp_loss,label='CASP12')\n",
    "plt.plot(range(1,num_epoch+1),TS115_loss,label='TS115')\n",
    "plt.plot(range(1,num_epoch+1),CB513_loss,label='CB513')\n",
    "plt.legend()\n",
    "plt.suptitle('Stage 2: Loss curves')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig('CNNLoss')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3: Traing both the VAE and CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stage 3 training: Training both the CNN and VAE simultaneously\n",
    "num_epochs = 100\n",
    "epoch = 0\n",
    "# Defining new loss function: NLLLoss\n",
    "# Not using PyTorch's cross entropy as the decoded latent variables already will be given as log probabilities.\n",
    "def criterion2(input, target):\n",
    "    labels = torch.argmax(target,2)\n",
    "    return nn.NLLLoss()(input, labels)\n",
    "\n",
    "# Creating lists to store losses and accuracies\n",
    "train_vaeLoss = []\n",
    "train_CNNLoss = []\n",
    "train_FinalLoss = []\n",
    "train_accuracy1 = []\n",
    "CASP_vaeLoss = []\n",
    "CASP_CNNLoss = []\n",
    "CASP_FinalLoss = []\n",
    "CASP_accuracy1 = []\n",
    "TS115_vaeLoss = []\n",
    "TS115_CNNLoss = []\n",
    "TS115_FinalLoss = []\n",
    "TS115_accuracy1 = []\n",
    "CB513_vaeLoss = []\n",
    "CB513_CNNLoss = []\n",
    "CB513_FinalLoss = []\n",
    "CB513_accuracy1 = []\n",
    "# training..\n",
    "while epoch < num_epochs:\n",
    "    epoch+= 1\n",
    "    print('Epoch ',epoch, ' of ', num_epochs)\n",
    "    training_epoch_data = defaultdict(list)\n",
    "    vae.train()\n",
    "    net.train()\n",
    "    running_loss_vae = 0.0\n",
    "    running_loss_CNN = 0.0\n",
    "    running_loss_FinalLoss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    print('Training...')\n",
    "    for data in TrainLoader:\n",
    "        inputs,labels = data\n",
    "        loss_vae, diagnostics, outputs = vi(vae, labels.permute(0,2,1))\n",
    "        running_loss_vae += loss_vae.data.numpy()\n",
    "        px, pz, qz, z = [outputs[k] for k in [\"px\", \"pz\", \"qz\", \"z\"]]\n",
    "        inputs, labels_net = Variable(inputs.permute(0,2,1)), Variable(z)\n",
    "        outputs_net = net(inputs)\n",
    "        px_pred = vae.observation_model(outputs_net.permute(0,2,1))[0].log_prob(labels.permute(0,2,1).permute(2,1,0).flatten(start_dim=1).permute(1,0)).view(labels.shape[0],labels.shape[2],labels.shape[1])\n",
    "        px_pred1 = vae.observation_model(outputs_net.permute(0,2,1))[0].log_prob(labels.permute(0,2,1).reshape(labels.shape[0]*labels.shape[2],labels.shape[1])).argmax(dim=1)\n",
    "        target = labels.permute(0,2,1).reshape(labels.shape[0]*labels.shape[2],labels.shape[1]).argmax(dim=1)\n",
    "        total += target.shape[0]\n",
    "        correct += torch.sum(px_pred1==target).data.numpy()\n",
    "        CNNLoss = criterion2(px_pred.permute(0,2,1),labels.detach().permute(0,2,1))\n",
    "        optimizer_net.zero_grad()\n",
    "        CNNLoss.backward\n",
    "        optimizer_net.step()\n",
    "        running_loss_CNN += CNNLoss.data.numpy()    \n",
    "        FinalLoss = CNNLoss.detach() + loss_vae\n",
    "        running_loss_FinalLoss += FinalLoss.data.numpy()\n",
    "        optimizer_vae.zero_grad()\n",
    "        FinalLoss.backward()\n",
    "        optimizer_vae.step()\n",
    "        # gather data for the current bach\n",
    "        for k, v in diagnostics.items():\n",
    "            training_epoch_data[k] += [v.mean().item()]\n",
    "    train_vaeLoss.append(running_loss_vae/len(TrainLoader))\n",
    "    train_CNNLoss.append(running_loss_CNN/len(TrainLoader))\n",
    "    train_FinalLoss.append(running_loss_FinalLoss/len(TrainLoader))\n",
    "    train_accuracy1.append(correct/total)\n",
    "\n",
    "    # Evaluate on a single batch with no backpropagation\n",
    "    with torch.no_grad():\n",
    "        vae.eval()\n",
    "        net.eval()\n",
    "        \n",
    "        print('Testing on CASP12')\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        # Loading a single batch from the test loader\n",
    "        x_casp,y_casp = next(iter(CASPLoader))\n",
    "        \n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        loss_casp, diagnostics_casp, outputs_casp = vi(vae, y_casp.permute(0,2,1))\n",
    "        outputs_casp = net(Variable(x_casp.permute(0,2,1)))\n",
    "        px_pred = vae.observation_model(outputs_casp.permute(0,2,1))[0].log_prob(y_casp.permute(0,2,1).permute(2,1,0).flatten(start_dim=1).permute(1,0)).view(y_casp.shape[0],y_casp.shape[2],y_casp.shape[1])\n",
    "        px_pred1 = vae.observation_model(outputs_casp.permute(0,2,1))[0].log_prob(y_casp.permute(0,2,1).reshape(y_casp.shape[0]*y_casp.shape[2],y_casp.shape[1])).argmax(dim=1)\n",
    "        target = y_casp.permute(0,2,1).reshape(y_casp.shape[0]*y_casp.shape[2],y_casp.shape[1]).argmax(dim=1)\n",
    "        total += target.shape[0]\n",
    "        correct += torch.sum(px_pred1==target).data.numpy()\n",
    "        CASP_vaeLoss.append(loss_casp.data.numpy()/len(CASPLoader))\n",
    "        CNNLoss = criterion2(px_pred.permute(0,2,1),y_casp.permute(0,2,1)).data.numpy()/len(CASPLoader)\n",
    "        CASP_CNNLoss.append(CNNLoss)\n",
    "        CASP_FinalLoss.append(CNNLoss + loss_casp.data.numpy()/len(CASPLoader))\n",
    "        CASP_accuracy1.append(correct/total)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        # Loading a single batch from the test loader\n",
    "        x_TS115,y_TS115 = next(iter(TS115Loader))\n",
    "\n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        loss_TS115, diagnostics_TS115, outputs_TS115 = vi(vae, y_TS115.permute(0,2,1))\n",
    "        outputs_TS115 = net(Variable(x_TS115.permute(0,2,1)))\n",
    "        px_pred = vae.observation_model(outputs_TS115.permute(0,2,1))[0].log_prob(y_TS115.permute(0,2,1).permute(2,1,0).flatten(start_dim=1).permute(1,0)).view(y_TS115.shape[0],y_TS115.shape[2],y_TS115.shape[1])\n",
    "        px_pred1 = vae.observation_model(outputs_TS115.permute(0,2,1))[0].log_prob(y_TS115.permute(0,2,1).reshape(y_TS115.shape[0]*y_TS115.shape[2],y_TS115.shape[1])).argmax(dim=1)\n",
    "        target = y_TS115.permute(0,2,1).reshape(y_TS115.shape[0]*y_TS115.shape[2],y_TS115.shape[1]).argmax(dim=1)\n",
    "        total += target.shape[0]\n",
    "        correct += torch.sum(px_pred1==target).data.numpy()\n",
    "        TS115_vaeLoss.append(loss_TS115.data.numpy()/len(TS115Loader))\n",
    "        CNNLoss = criterion2(px_pred.permute(0,2,1),y_TS115.permute(0,2,1)).data.numpy()/len(TS115Loader)\n",
    "        TS115_CNNLoss.append(CNNLoss)\n",
    "        TS115_FinalLoss.append(CNNLoss + loss_TS115.data.numpy()/len(TS115Loader))\n",
    "        TS115_accuracy1.append(correct/total)\n",
    "        \n",
    "        print('Testing on CB513')\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        # Loading a single batch from the test loader\n",
    "        x_CB513,y_CB513= next(iter(CB513Loader))\n",
    "\n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        loss_CB513, diagnostics_CB513, outputs_CB513 = vi(vae, y_CB513.permute(0,2,1))\n",
    "        outputs_CB513 = net(Variable(x_CB513.permute(0,2,1)))\n",
    "        px_pred = vae.observation_model(outputs_CB513.permute(0,2,1))[0].log_prob(y_CB513.permute(0,2,1).permute(2,1,0).flatten(start_dim=1).permute(1,0)).view(y_CB513.shape[0],y_CB513.shape[2],y_CB513.shape[1])\n",
    "        px_pred1 = vae.observation_model(outputs_CB513.permute(0,2,1))[0].log_prob(y_CB513.permute(0,2,1).reshape(y_CB513.shape[0]*y_CB513.shape[2],y_CB513.shape[1])).argmax(dim=1)\n",
    "        target = y_CB513.permute(0,2,1).reshape(y_CB513.shape[0]*y_CB513.shape[2],y_CB513.shape[1]).argmax(dim=1)\n",
    "        total += target.shape[0]\n",
    "        correct += torch.sum(px_pred1==target).data.numpy()\n",
    "        CB513_vaeLoss.append(loss_CB513.data.numpy()/len(CB513Loader))\n",
    "        CNNLoss = criterion2(px_pred.permute(0,2,1),y_CB513.permute(0,2,1)).data.numpy()/len(CB513Loader)\n",
    "        CB513_CNNLoss.append(CNNLoss)\n",
    "        CB513_FinalLoss.append(CNNLoss + loss_CB513.data.numpy()/len(CB513Loader))\n",
    "        CB513_accuracy1.append(correct/total)\n",
    "plt.plot(range(1,num_epoch+1),train_CNNLoss,label='Train')\n",
    "plt.plot(range(1,num_epoch+1),CASP_CNNLoss,label='CASP12')\n",
    "plt.plot(range(1,num_epoch+1),TS115_CNNLoss,label='TS115')\n",
    "plt.plot(range(1,num_epoch+1),CB513_CNNLoss,label='CB513')\n",
    "plt.legend()\n",
    "plt.suptitle('Stage 3: Loss curves')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig('CNNLossStage3')\n",
    "plt.close()\n",
    "plt.plot(range(1,num_epoch+1),train_accuracy1,label='Train')\n",
    "plt.plot(range(1,num_epoch+1),CASP_accuracy1,label='CASP12')\n",
    "plt.plot(range(1,num_epoch+1),TS115_accuracy1,label='TS115')\n",
    "plt.plot(range(1,num_epoch+1),CB513_accuracy1,label='CB513')\n",
    "plt.legend()\n",
    "plt.suptitle('Stage 3: Accuracies')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.savefig('CNNaccuracyStage3')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
